# Hands-on Lab: Create a DAG for Apache Airflow

![cognitiveclass.ai logo](cognitiveclass_logo.png)

Estimated time needed: 40 minutes

## Objectives

After completing this lab you will be able to:

- Explore the anatomy of a DAG.
- Create a DAG.
- Submit a DAG.

# Exercise 1 : Start Apache Airflow

```bash
start_airflow
```
# Exercise 2 : Open the Airflow Web UI

Here's the text formatted in Markdown with collapsible sections for each block:

# Exercise 3 - Explore the anatomy of a DAG

An Apache Airflow DAG is a Python program. It consists of these logical blocks.

<details>
<summary><b>Imports</b></summary>

A typical imports block looks like this:

```python
# import the libraries
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to write tasks!
from airflow.operators.bash_operator import BashOperator
# This makes scheduling easy
from airflow.utils.dates import days_ago
```
</details>

<details>
<summary><b>DAG Arguments</b></summary>

A typical DAG Arguments block looks like this:

```python
#defining DAG arguments
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'Ramesh Sannareddy',
    'start_date': days_ago(0),
    'email': ['ramesh@somemail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

DAG arguments are like settings for the DAG. They specify things like the owner name, start date, email alerts settings, retries, and retry delay.
</details>

<details>
<summary><b>DAG Definition</b></summary>

A typical DAG definition block looks like this:

```python
# define the DAG
dag = DAG(
    dag_id='sample-etl-dag',
    default_args=default_args,
    description='Sample ETL DAG using Bash',
    schedule_interval=timedelta(days=1),
)
```

Here we are creating a variable named `dag` by instantiating the DAG class with parameters such as `dag_id`, `default_args`, `description`, and `schedule_interval`.
</details>

<details>
<summary><b>Task Definitions</b></summary>

A typical task definitions block looks like this:

```python
# define the tasks
# define the first task named extract
extract = BashOperator(
    task_id='extract',
    bash_command='echo "extract"',
    dag=dag,
)
# define the second task named transform
transform = BashOperator(
    task_id='transform',
    bash_command='echo "transform"',
    dag=dag,
)
# define the third task named load
load = BashOperator(
    task_id='load',
    bash_command='echo "load"',
    dag=dag,
)
```

A task is defined using a `task_id`, the bash command it represents, and which DAG this task belongs to.
</details>

<details>
<summary><b>Task Pipeline</b></summary>

A typical task pipeline block looks like this:

```python
# task pipeline
extract >> transform >> load
```

Task pipeline helps us to organize the order of tasks. Here the task `extract` must run first, followed by `transform`, followed by the task `load`.
</details>

